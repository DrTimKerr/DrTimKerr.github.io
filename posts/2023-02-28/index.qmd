---
title: "Plan for March"
author: "Tim Kerr"
editor: visual
---

### Progress Thus Far

I have a set of data I can work with.

I have built variants of a reinforcement learning model in Stan.

This model works, appears to extract individual differences in parameters, generates data, and offers an information criterion.

I have correlated estimated parameters to phenotype data, and run a frequentist significance test.

### Problems

The data are untidy, and I cannot make inferences from them. It is not discretised to individual studies. There are missing data such as scream patterns and CS+/CS- orders.

The models built are complex, have many parameters, and require some significant data transformations.

The models don't work well, they are rigid. They do not capture much of the volatility in the data. The generated data distributions are uninformative. The information criterions are bonkers, preventing me from comparing models.

A Bayesian significance test should be used given the model. In addition, null models and other sanity checks should be used.

### The Correct Pipeline per the Literature

Referencing this Wilson & Collins (2019) [paper](https://elifesciences.org/articles/49547)

1)  Theorise a set of models which all have a chance to realistically capture the data. This can include null models such as random button pressing. (Also just fit a curve)

2)  

i)  Simulate data. Using the models, set the free parameters via a random choice from a distribution, and generate simulated data. Then run this data back through the model, to see if it can **recover** the parameters correctly. Do this several times iteratively, to see if the models can cope with many parameter settings, which is indicative that they can actually estimate free parameters from real data. (Run a KL divergance score here?)


ii) Plot the correlation between real and recovered parameters, looking for a correlation above a certain level, >0.5 correlation say. Plotting will show biases or areas where there is large divergance.

nb- some models will only work within certain parameter ranges, so completely random parameters may not work, and might need to be bounded within realistic limits.

3)  Compare different models using an information criterion, or cross validation. This can be plotted in a confusion matrix, which validates model comparison via model recovery on simulated data. (Is simulated data from model A best fit by model A, or model B, C etc. per BIC/WAIC). Confusion matrix should be an identity matrix. If not, look into running an inversion matrix.

4)  Run models with real data. You can further simulate to find biases, such as left/right biases, or optimism biases, which affect choice above basic model. Validation should be via a posterior predictive check, i.e. by simulating data with the fit parameter values. This leads to a winning model.

5)  Extract latent variables from the model. These can be correlated with physiological data, especially their evolution over time. You can also study individual differences, using fit parameters as dependent variable in continuous analyses, correlating with age or symptoms, or group comparisons.

### Learning Still to do

Log Likelihood and model comparisons therein. I still don't quite understand exactly what this is measuring, and hence why the model comparisons aren't working. I have found a lecture [series](https://github.com/rmcelreath/stat_rethinking_2023) which explains this in more detail than Lei Zhang's course.

Dirichlet distributions for discrete data. I think this is a better fit for the experiment than a beta distribution alone. This tutorial might explain [things](https://www.alexpghayes.com/post/2018-12-24_some-things-ive-learned-about-stan/)

### Relevant Papers

Hopkins, A. K., Dolan, R., Button, K. S., & Moutoussis, M. (2021). A Reduced Self-Positive Belief Underpins Greater Sensitivity to Negative Evaluation in Socially Anxious Individuals. Computational Psychiatry, 5(1), 21--37. DOI: http://doi.org/10.5334/cpsy.57 [paper](https://cpsyjournal.org/article/10.5334/cpsy.57/) [supplement](https://s3-eu-west-1.amazonaws.com/ubiquity-partner-network/up/journal/cpsy/cpsy-5-1-57-s1.PDF) 

This paper contains beta distributions, and aims to explain anxious participants learning more from negative experiences. Good clear explanations in supplement.

Wise T, Michely J, Dayan P, Dolan RJ (2019) A computational account of threat-related attentional bias. PLoS Comput Biol 15(10): e1007341. https://doi.org/10.1371/journal.pcbi.1007341 [paper](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007341)

This paper uses 'leaky beta distributions', I think in place of RL. It also examines attentional processes, via a Pierce-Hall adaptation, which might better model decay/boredom.

### Plan

1)  Build a set of models in R, to simulate data.

a)  Null - random button pressing
b)  Null - single button pressing
c)  Curve - polynomial regression
d)  RL - RW/Beta two parameters (alpha (LR) and sigma (variance))
e)  RL - RW/Beta three parameters (reward alpha and punishment alpha (LR) and sigma (variance))
f)  RL - RW/Dirichet two parameters
g)  RL - RW/Dirichet three parameters


2)  Rebuild above in Stan, and compare these models per Wilson paper, including correlations and confusion matrices.

3) Make a better real dataset to work from, perhaps going back to source to find correct patterns. :cold_sweat: 

4) Run the best models with real data.

5) Possibly look to pre-register at this point, prior to running correlations with phenotype data. 

### Timeframe

There are no major obstacles or life events in March. I have four relatively clear weeks to do this work, prior to running the Paris Marathon on April 2nd, where I will take a week off post. 

This week, i.e. by Friday 3rd, I hope to build the R models.

In the week commencing 6th March, I will rebuild these in Stan, and start model comparisons.

W/C 13th March, I will apply these to the real data, if on schedule.

W/C 20th March, I will continue to apply these to real data.

W/C 27th March, I will write up the pre-registration.

Longer term, I hope to present results in June, and a poster in July.


