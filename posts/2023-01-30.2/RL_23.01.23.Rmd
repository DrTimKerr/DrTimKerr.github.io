---
title: "RL_30.01.23"
author: "Tim Kerr"
date: "`r Sys.Date()`"
output: html_document
---

Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# setwd("/Users/timkerr/FLARe/RW_05.01.23") # Set working directory

palette.colors(palette = "Okabe-Ito") # colourblind palette

library(tidyverse) # for any data manipulation

library(rstan) # for Stan

# install.packages("hBayesDM", dependencies=TRUE)

# library(hBayesDM)

library(loo)

library(scales)
```

Functions

```{r}

run_stan <- function(file, datalist) {
  rstan_options(auto_write = TRUE)
  options(mc.cores = 4)
  nchains <- 4
  
  file <- paste('./Stan/',file,sep = "")
  
  cat('Estimating',file, 'model ... \n')
  starttime <- Sys.time(); print(starttime)
  cat('Calling', nchains, 'simulations in Stan ... \n')
  
  fit <- stan(file,
                 data = datalist,
                 chains = 4,
                 iter = 2000,
                 warmup = 1000,
                 init = 'random',
                 seed = 45678)
  
  cat('Finishing', file, 'model simulation ... \n')
  endtime <- Sys.time(); print(endtime)
  cat('It took',as.character.Date(endtime - starttime), '\n')
  return(fit)
}

scale_er <- function(x){
  
  vals <- seq(0.5/9,1,1/9)
  for (n in 1:length(x)){
    for (val in 1:9){
      if (x[n] > val-1 & x[n] <= val){
        x[n] <- vals[val]
      # x <- 1 - vals[val] #### to invert, not sure if needed ####
    }
   }
  }
  return(x)
}

```

Data

```{r}

# setwd("./Users/timkerr/FLARe/RW_05.01.23") # Set working directory

plus <- read_csv("/Users/timkerr/FLARe/RW_05.01.23/Data/acq_plus_335.csv", col_names = FALSE, show_col_types = FALSE) # all CS+ rating scores, row = participant, column = trial number 1-12

minus <- read_csv("/Users/timkerr/FLARe/RW_05.01.23/Data/acq_minus_335.csv", col_names = FALSE, show_col_types = FALSE) # all CS- rating scores, as above

scream_plus <- read_csv("/Users/timkerr/FLARe/RW_05.01.23/Data/bayes_screams_acq.csv", col_names = FALSE, show_col_types = FALSE) # all screams (US), 1 = scream, 0 = no scream


set.seed(28397456)
random10 <- sample(1:335, 10, replace=FALSE) # 10 random numbers as vector

plus_sample <- plus[random10,] # 10 participants from CS+ dataset
minus_sample <- minus[random10,] # the same 10 participants from CS+ dataset
scream_plus_sample <- scream_plus[random10,] # the corroborating scream (US) data from same 10 participants 

scream_minus_sample <- matrix(0L,nrow = 10, ncol = 12) # no scream data for CS- sample

single_subject_expectancy <- slice(plus_sample, 1)
choice <- as.double(single_subject_expectancy[1,])
choice_scaled <- scale_er(choice)
reward <- as.double(slice(scream_plus_sample, 1))
ntrials <- length(choice)
nsub <- 1

datalist_single <- list(ntrials=ntrials,
                 nsub=nsub,
                 choice=choice_scaled,
                 reward=reward)

multi_subject_expectancy <- as.matrix(plus_sample)
choice_scaled <- scale_er(multi_subject_expectancy)
reward <- as.matrix(scream_plus_sample)
ntrials <- dim(multi_subject_expectancy)[2]
nsub <- dim(multi_subject_expectancy)[1]

datalist_multi <- list(ntrials=ntrials,
                 nsub=nsub,
                 choice=choice_scaled,
                 reward=reward)

```

# Introduction

To use computational / cognitive modelling approaches to extract more parameters from FLARe data, which can later be used to a) find differences between measurements of groups (in validation study), and/or b) to correlate with genetic or phenotypic data from TEDS and/or GLAD.

The software used to build models and extract parameters is Stan. Stan allows one to program bayesian models, capturing and using the uncertainty within parameter estimation (rather than using means per frequentist approaches).

A simple example is using Stan to estimate the probability of a biased coin. If we flip a coin 12 times, and see 9 heads, we might assume the pribability of heads is 0.75. However, this is based on limited data, and you cant measure/show the uncertainty.

You want to know the probability of the coin being biased to 0.75 heads, given the data of 9 heads in 12 trials. Theta represents the model, or (unknown) probability.

i.e. $p(Heads\mid N,\theta)$

And the number of heads will pertain to the binomial distribution:

$heads \sim Binomial(N,\theta)$

```{stan output.var="heads", echo = TRUE}

data { //known observation data
  int<lower=0> h;
  int<lower=0> N;
}

parameters {
  real<lower=0, upper=1> theta;
}

model {
  // theta ~ uniform(0,1);
  h ~ binomial(N, theta);
}

generated quantities {
  real log_lik;
  int heads;
  
  log_lik = binomial_lpmf(h | N, theta);
  heads = binomial_rng(N, theta);
  
}
```

Stan uses Markov Chain Monte Carlo sampling, to estimate the parameter. One chain is run per CPU core. Each chain samples a point on the distribution. It is rewarded for samples near the peak/mean of the distribution. It then moves to another area of the distribution. In this instance, it does this 1000 times. Given the reward, it thus converges around this mean, which you can see in the trace plot.

This probability density function (PDF), graphically shows the uncertainty contained within the parameter estimate. The red area denotes the highest density interval (HDI), set here to its default of 0.8/80%. If more data were introduced, the uncertainty would reduce.

Stan can also generate data based upon the parameter estimate, from the 4\*1000 samples. The histogram shows 9 heads being the most likely based on the 4\*1000 samples.

```{r}

h <- 9
N <- 12

data <- list(h=h,
             N=N)

heads_fit <- rstan::sampling(heads,
                             data = data)

rstan::summary(heads_fit)

stan_plot(heads_fit, show_density = TRUE, pars = 'theta')

stan_trace(heads_fit, pars = 'theta')

heads <- extract(heads_fit, pars = 'heads')

hist(heads$heads, xaxp = c(0,12,12))

```

Now to look at FLARe data, which, while more complex than a coin flip, shares some similarities. There is an unknown parameter to the participants, the reinforcement rate of the US (75%). They experience the US as happening or not happening, so could be said to be binomial in distribution. This happens over 12 trials.

The first plot shows 10 participants individual ratings over 12 trials, and the group mean in black. This individual variation is the thing I hope models can explain.

The second plot shows a single participant. This participant appears to do the experiment correctly, and reacts as expected to US. It also appears similar to the mean.

For now, for simplicity, I am just going to model the CS+ acquisition.

```{r}

## Create a tibble to plot the 10 participants. Some manipulation of variable type to make plotting easier, such as subject number as factor so colours are distinct on graph, rather than a gradient which is the default for continuous data or integer type. Trial number as integer so it sorts the x axis in numerical order. You can make it factor and specify the order with a vector, but I dont want to type out all that. 

plus_plot <- plus_sample %>% 
  add_column(subject = 1:10, .before = TRUE) %>% 
  pivot_longer(
  !subject,
  names_to = 'trial',
  values_to = 'rating') 
plus_plot$trial <- gsub('X','',as.character(plus_plot$trial))
plus_plot$trial <- as.integer(plus_plot$trial)
plus_plot$subject <- as.factor(plus_plot$subject)

## Separate tibble to plot the mean and sd of the 10 participants. 

mean_plus_plot <- plus_plot %>% 
  group_by(trial) %>% 
  summarise(mean_expectancy_rating_per_trial = mean(rating, na.rm = TRUE), sd_expectancy_rating_per_trial = sd(rating, na.rm = TRUE))

## Ungroup?

## Better attempt at graphing from two tibbles. Not sure how to fix error bars yet. 
  
ggplot(NULL) +
  geom_point(data = plus_plot, aes(x = trial, y = rating, group = subject, colour = subject)) +
  geom_line(data = plus_plot, aes(trial, rating, group = subject, colour = subject)) +
  scale_x_continuous(breaks = seq(1,12,1)) +
  scale_y_continuous(breaks = seq(1,9,1)) +
  geom_line(data = mean_plus_plot, aes(x = trial, y = mean_expectancy_rating_per_trial)) +
  geom_point(data = mean_plus_plot, aes(x = trial, y = mean_expectancy_rating_per_trial), size = 3)

## Plotting a single subject

single_subject <- plus_sample %>% 
  slice(1) %>% 
  add_row(slice(minus_sample, 1)) %>% 
  add_row(slice(scream_plus_sample, 1)) %>% 
  add_column(stimulus = c('plus','minus','scream'), .before = TRUE) %>% 
  pivot_longer(
    !stimulus,
    names_to = 'trial',
    values_to = 'rating'
  )
single_subject$trial <- gsub('X','',as.character(single_subject$trial))
single_subject$trial <- as.integer(single_subject$trial)

scream_vector <- as.numeric(slice(scream_plus_sample, 1))
scream_vector[scream_vector == 0] <- NA

ggplot(NULL) +
  geom_point(data = subset(single_subject, stimulus == c('plus')), aes(x = trial, y = rating, group = stimulus, colour = stimulus)) +
  geom_line(data = subset(single_subject, stimulus == c('plus')), aes(trial, rating, group = stimulus, colour = stimulus)) +
  geom_point(data = subset(single_subject, stimulus == c('minus')), aes(x = trial, y = rating, group = stimulus, colour = stimulus)) +
  geom_line(data = subset(single_subject, stimulus == c('minus')), aes(trial, rating, group = stimulus, colour = stimulus)) +
  scale_x_continuous(breaks = seq(1,12,1)) +
  scale_y_continuous(breaks = seq(1,9,1)) +
  geom_point(data = subset(single_subject, stimulus == c('scream')), aes(x = trial, y = 10*scream_vector), shape = "\u2020", size = 10) +

# unicode u26A1 is lightning bolt
# 
  geom_line(data = mean_plus_plot, aes(x = trial, y = mean_expectancy_rating_per_trial)) +
  geom_point(data = mean_plus_plot, aes(x = trial, y = mean_expectancy_rating_per_trial), size = 3)

```

------------------------------------------------------------------------

# Simulation of RW and learning rates in R

Simple graphs of RW updating.

Top left graph is the actual data, plotted.

The three other graphs show the value updating, given low, medium, and high learning rates.

# The RW model

$\alpha$ - Learning rate (free parameter)

$PE$ - reward prediction error (reward - current expectation)

$V$ - value (subjective)

$R$ - reward (US = 1, no US = 0)

$t$ - trial (1,2,...,12)

$\text{Value Update: } V_t = V_{t-1}+\alpha*PE_{t-1}$

$\text{Prediction Error: } PE_{t-1} = R_{t-1} - V_{t-1}$

$V_t = V_{t-1} + \alpha(R_{t-1} - V_{t-1})$

```{r}

update_RW <- function(value, alpha, beta=1, lambda) {
  value_compound <- sum(value)                    # value of the compound 
  prediction_error <- lambda - value_compound     # prediction error
  value_change <- alpha * beta * prediction_error # change in strength
  value <- value + value_change                   # update value
  return(value)
}

alpha <- c(0.2,0.5,0.8)

lambda = as.double(slice(scream_plus_sample, 1))*9 # *9 because I havent scaled value
# lambda = ifelse(lambda == 0, 5, 9) # changing punishment sensitivity (more like ER of 5 being neutral)


n_trials <- 12 

# strength <- numeric(n_trials)
strength <- as.double(slice(plus_sample, 1))
ER <- strength
strength1 <- strength
strength2 <- strength
strength3 <- strength

for(trial in 2:n_trials) {
  strength1[trial] <- update_RW( strength1[trial-1], alpha = alpha[1], lambda = lambda[trial-1] )
}
for(trial in 2:n_trials) {
  strength2[trial] <- update_RW( strength2[trial-1],alpha = alpha[2], lambda = lambda[trial-1] )
}
for(trial in 2:n_trials) {
  strength3[trial] <- update_RW( strength3[trial-1],alpha = alpha[3], lambda = lambda[trial-1] )
}


par(mfrow=c(2,2)) 
plot(ER,
  xlab = "Trial Number",
  ylab = "Expectancy",
  ylim = c(0,9),
  type = "b", 
  pch = 19,
 
  )
plot(
  strength1, 
  xlab = "'Learning Rate = 0.2",
  ylab = "Association Value",
  ylim = c(0,9),
  type = "b", 
  pch = 19
)
plot(
  strength2, 
  xlab = "'Learning Rate = 0.5",
  ylab = "Association Value",
  ylim = c(0,9),
  type = "b", 
  pch = 19
)
plot(
  strength3, 
  xlab = "'Learning Rate = 0.8",
  ylab = "Association Value",
  ylim = c(0,9),
  type = "b", 
  pch = 19
)


```

------------------------------------------------------------------------

# Data processing steps

Expectancy rating of 1, 2, ..., 9 is categorical. I have scaled this to a continuous distribution between 0 and 1, to represent (participant's subjective) probability.

Scale is 9 steps between $\frac{0.5}{9}$ and $1$ in $\frac{1}{9}$ increments.

US / Scream is considered a 'reward', and has value of $1$. No US has a value of $0$.

------------------------------------------------------------------------

# Model 1 - Rescorla Wagner, Choice \~ Normal(value,sigma)

### Model and Parameters

RW value updating, via learning rate parameter $lr$.

Value mapped to participant choice / expectancy rating via a normal distribution. Mean $\mu$ is value for that trial. Sigma $\sigma$ is estimated by the model.

### Outputs

Log Likelihood extracted per participant and trial. Leave one out cross validation (LOOCV) and WAIC calculated from this. (For model comparison)

Histogram of generated data for trial 9, showing normal distribution.

Final plot is mean of generated choices from model in blue, and actual choice data, all from participant one. Not sure how to use this data yet, possibly some sort of divergance score.

```{stan output.var="normal", echo = TRUE}

data {
  int<lower=1> ntrials;          
  int<lower=1> nsub;
  real<lower=0,upper=1> choice[nsub,ntrials]; //array of size nTrials (12 intigers)
  real<lower=-1, upper=1> reward[nsub,ntrials]; 
}

parameters {
  real<lower=0,upper=1> lr[nsub];
  real<lower=0> sigma[nsub];  
}

model {

  real v[nsub,ntrials];
  real pe[nsub,ntrials];       // prediction error
  
  for (s in 1:nsub) {
  v[s,1] = 0.5;

  for (t in 1:(ntrials-1)) { 
    
    pe[s,t] = reward[s,t] - v[s,t];

    v[s,t+1] = v[s,t] + lr[s] * pe[s,t]; 
  }  
  
  for (t in 1:ntrials) {

    choice[s,t] ~ normal (v[s,t],sigma[s]);
  }
}
}

generated quantities {
  real log_lik[nsub,ntrials];
  real choice_pred[nsub,ntrials];
  
  choice_pred = rep_array(-999,nsub,ntrials);
  
  {
  real v[nsub,ntrials];
  real pe[nsub,ntrials];      
  
  for (s in 1:nsub) {
  v[s,1] = 0.5;

  for (t in 1:(ntrials-1)) { 
    
    pe[s,t] = reward[s,t] - v[s,t];

    v[s,t+1] = v[s,t] + lr[s] * pe[s,t]; 
  }  
  
  for (t in 1:ntrials) {
    log_lik[s,t] = normal_lpdf(choice[s,t] | v[s,t],sigma[s]);
    choice_pred[s,t] = normal_rng(v[s,t],sigma[s]);
  }
}
  }
}


```


```{r}


fit_normal <- rstan::sampling(normal,
                             data = datalist_multi)

# print(summary(fit_normal))

stan_trace(fit_normal, pars = 'lr[1]')

# stan_plot(fit_rw, show_density = TRUE)
stan_plot(fit_normal, show_density = TRUE, pars = 'lr')
stan_plot(fit_normal, show_density = TRUE, pars = 'sigma')



LL1 <- extract_log_lik(fit_normal)
loo_normal <- loo(LL1)
waic_normal <- waic(LL1)

gen_data <- extract(fit_normal)
gen_data <- gen_data$choice_pred

hist(gen_data[,1,9])

gen_data_plot <- c()
for (i in 1:12) {
  gen_data_plot[i] <- mean(gen_data[,1,i])
}
plot(gen_data_plot, ylim = c(0,1), col = 'blue')
par(new = TRUE)
plot(choice_scaled[1,],ylim = c(0,1), col = 'red')


```

# Model 2 - Rescorla Wagner, Choice \~ Beta(alpha,beta)

### Model and Parameters

RW value updating, via learning rate parameter $lr$.

Value mapped to participant choice / expectancy rating via a beta distribution. Mean $\mu$ is value for that trial. Sigma $\sigma$ is estimated by the model. (and called beta)

Beta distribution is more flexible than normal. It is governed by two shaping parameters, alpha and beta. These can be derived from mean and variance:

$\alpha = \mu(\frac{\mu(1 - \mu)}{\sigma - 1})$

$\beta = (1 - \mu)(\frac{\mu(1 - \mu)}{\sigma - 1})$

### Outputs

As model 1. Log Likelihood extracted per participant and trial. Leave one out cross validation (LOOCV) and WAIC calculated from this. (For model comparison)

Histogram showing generated data of trial 9, showing beta distribution, skewed towards high probability. 

Final plot is mean of generated choices from model in blue, and actual choice data, all from participant one.

```{stan output.var="beta", echo = TRUE}

data {
  int<lower=1> ntrials;          
  int<lower=1> nsub;
  real<lower=0,upper=1> choice[nsub,ntrials]; //array of size nTrials (12 intigers)
  real<lower=-1, upper=1> reward[nsub,ntrials]; 
}

parameters {
  real<lower=0,upper=1> lr[nsub];
  real<lower=0> beta[nsub];  
}

model {

  real v[nsub,ntrials];
  real pe[nsub,ntrials];       // prediction error
  
  real shape_alpha[nsub,ntrials];
  real shape_beta[nsub,ntrials];
  
  //beta ~ normal(0,1);
  
  for (s in 1:nsub) {
  v[s,1] = 0.5;

  for (t in 1:(ntrials-1)) { 
    
    pe[s,t] = reward[s,t] - v[s,t];

    v[s,t+1] = v[s,t] + lr[s] * pe[s,t]; 
  }  
  
  for (t in 1:ntrials) {
    
    shape_alpha[s,t] = v[s,t] * ((v[s,t] * (1-v[s,t]) / beta[s]));
    shape_beta[s,t] = (1-v[s,t]) * ((v[s,t] * (1-v[s,t]) / beta[s]));
    
    choice[s,t] ~ beta(shape_alpha[s,t],shape_beta[s,t]);
  }
}
}

generated quantities {
  real log_lik[nsub,ntrials];
  real choice_pred[nsub,ntrials];
  
  choice_pred = rep_array(-999,nsub,ntrials);
  
  {
  real v[nsub,ntrials];
  real pe[nsub,ntrials];      
  
  real shape_alpha[nsub,ntrials];
  real shape_beta[nsub,ntrials];
  
  for (s in 1:nsub) {
  v[s,1] = 0.5;

  for (t in 1:(ntrials-1)) { 
    
    pe[s,t] = reward[s,t] - v[s,t];

    v[s,t+1] = v[s,t] + lr[s] * pe[s,t]; 
  }  
  
  for (t in 1:ntrials) {
    shape_alpha[s,t] = v[s,t] * ((v[s,t] * (1-v[s,t]) / beta[s]));
    shape_beta[s,t] = (1-v[s,t]) * ((v[s,t] * (1-v[s,t]) / beta[s]));
    
    log_lik[s,t] = beta_lpdf(choice[s,t] | shape_alpha[s,t], shape_beta[s,t]);
    choice_pred[s,t] = beta_rng(shape_alpha[s,t], shape_beta[s,t]);
  }
}
  }
}


```


```{r}


fit_beta <- rstan::sampling(beta,
                             data = datalist_multi)

# print(summary(fit_beta))

stan_trace(fit_beta)

# stan_plot(fit_rw, show_density = TRUE)
stan_plot(fit_beta, show_density = TRUE, pars = 'lr')
# stan_plot(fit_beta, show_density = TRUE, pars = c('lr[1]','beta[1]'))
stan_plot(fit_beta, show_density = TRUE, pars = 'beta')



LL1 <- extract_log_lik(fit_beta)
loo_beta <- loo(LL1)
waic_beta <- waic(LL1)

gen_data <- extract(fit_beta)
gen_data <- gen_data$choice_pred

hist(gen_data[,1,9])

gen_data_plot <- c()
for (i in 1:12) {
  gen_data_plot[i] <- mean(gen_data[,1,i])
}
plot(gen_data_plot, ylim = c(0,1), col = 'blue')
par(new = TRUE)
plot(choice_scaled[1,],ylim = c(0,1), col = 'red')

```

Compare the two models. Beta model has lower WAIC, so is the superior model.

```{r, echo = FALSE}

loo_compare(loo_normal,loo_beta)

barplot(c(waic_normal$waic, waic_beta$waic), xlab = 'WAIC', names.arg = c('Normal','Beta'))

waic_df <- data.frame(model = c('0Normal','Beta'), WAIC = c(waic_normal$waic,waic_beta$waic))

ggplot(data = waic_df, aes(x = model, y = WAIC))  + geom_bar(stat = 'identity')

```

<!-- Ridge plots to show estimated distribution density. Shows the advantage of beta distribution over normal, which is inflexible and inappropriate given upper bounds of probability at 1. -->

<!-- ```{r} -->

<!-- gen_data <- extract(fit_normal) -->
<!-- gen_data <- gen_data$choice_pred -->

<!-- library(ggridges) -->
<!-- ggplot(gen_data_tibble, aes(x = gen_data_tibble[,1,], y = gen_data_tibble[,1,1:12])) + geom_density_ridges() -->

<!-- gen_data_tib <- as.tibble(gen_data[,1,]) -->
<!-- tib2 <- gen_data_tib %>% pivot_longer(cols = starts_with('V'), names_to = 'Trial', values_to = 'Rating') -->
<!-- tib2$Trial <- gsub('V10','W10',as.character(tib2$Trial)) -->
<!-- tib2$Trial <- gsub('V11','W11',as.character(tib2$Trial)) -->
<!-- tib2$Trial <- gsub('V12','W12',as.character(tib2$Trial)) -->
<!-- # tib2$Trial <- as.integer(tib2$Trial) -->

<!-- single_subject_expectancy_tib <- single_subject_expectancy %>% as.tibble() %>% pivot_longer(cols = starts_with('X'), names_to = 'Trial', values_to = 'Rating') -->
<!-- single_subject_expectancy_tib$Trial <- gsub('X10','Y10',as.character(single_subject_expectancy_tib$Trial)) -->
<!-- single_subject_expectancy_tib$Trial <- gsub('X11','Y11',as.character(single_subject_expectancy_tib$Trial)) -->
<!-- single_subject_expectancy_tib$Trial <- gsub('X12','Y12',as.character(single_subject_expectancy_tib$Trial)) -->

<!-- ggplot(tib2, aes(x = Trial, y = Rating, group = Trial)) + geom_density_ridges(aes(fill = Trial)) -->

<!-- choice_1 <- scale_er(choice) -->
<!-- choice_1_df <- data.frame(Trial = c('V1','V2','V3','V4','V5','V6','V7','V8','V9','W10','W11','W12'), Rating = choice_1 )  -->

<!-- ggplot(tib2, aes(y = Trial, x = Rating)) + geom_density_ridges(scale = 1) + geom_point(data = choice_1_df, col = 'blue') + xlim(0,1.2) -->

<!-- order <- c('V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12') -->

<!-- mean(gen_data_tib$V2) -->

<!-- ``` -->

<!-- ```{r} -->

<!-- gen_data <- extract(fit_beta) -->
<!-- gen_data <- gen_data$choice_pred -->

<!-- library(ggridges) -->
<!-- ggplot(gen_data_tibble, aes(x = gen_data_tibble[,1,], y = gen_data_tibble[,1,1:12])) + geom_density_ridges() -->

<!-- gen_data_tib <- as.tibble(gen_data[,1,]) -->
<!-- tib2 <- gen_data_tib %>% pivot_longer(cols = starts_with('V'), names_to = 'Trial', values_to = 'Rating') -->
<!-- tib2$Trial <- gsub('V10','W10',as.character(tib2$Trial)) -->
<!-- tib2$Trial <- gsub('V11','W11',as.character(tib2$Trial)) -->
<!-- tib2$Trial <- gsub('V12','W12',as.character(tib2$Trial)) -->
<!-- # tib2$Trial <- as.integer(tib2$Trial) -->

<!-- single_subject_expectancy_tib <- single_subject_expectancy %>% as.tibble() %>% pivot_longer(cols = starts_with('X'), names_to = 'Trial', values_to = 'Rating') -->
<!-- single_subject_expectancy_tib$Trial <- gsub('X10','Y10',as.character(single_subject_expectancy_tib$Trial)) -->
<!-- single_subject_expectancy_tib$Trial <- gsub('X11','Y11',as.character(single_subject_expectancy_tib$Trial)) -->
<!-- single_subject_expectancy_tib$Trial <- gsub('X12','Y12',as.character(single_subject_expectancy_tib$Trial)) -->

<!-- ggplot(tib2, aes(x = Trial, y = Rating, group = Trial)) + geom_density_ridges(aes(fill = Trial)) -->

<!-- choice_1 <- scale_er(choice) -->
<!-- choice_1_df <- data.frame(Trial = c('V1','V2','V3','V4','V5','V6','V7','V8','V9','W10','W11','W12'), Rating = choice_1 )  -->

<!-- ggplot(tib2, aes(y = Trial, x = Rating)) + geom_density_ridges(scale = 1) + geom_point(data = choice_1_df, col = 'blue') + xlim(0,1.2) -->

<!-- order <- c('V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12') -->

<!-- mean(gen_data_tib$V2) -->
<!-- ``` -->
