{
  "hash": "f4668ee5e52ec03d76183ad6dadcf4ba",
  "result": {
    "markdown": "---\ntitle: \"RL_30.01.23\"\nauthor: \"Tim Kerr\"\ndate: \"2023-01-30\"\noutput: html_document\n---\n\n\nSetup\n\n\n\n\n\nFunctions\n\n\n::: {.cell}\n\n:::\n\n\nData\n\n\n::: {.cell}\n\n:::\n\n\n# Introduction\n\nTo use computational / cognitive modelling approaches to extract more parameters from FLARe data, which can later be used to a) find differences between measurements of groups (in validation study), and/or b) to correlate with genetic or phenotypic data from TEDS and/or GLAD.\n\nThe software used to build models and extract parameters is Stan. Stan allows one to program bayesian models, capturing and using the uncertainty within parameter estimation (rather than using means per frequentist approaches).\n\nA simple example is using Stan to estimate the probability of a biased coin. If we flip a coin 12 times, and see 9 heads, we might assume the pribability of heads is 0.75. However, this is based on limited data, and you cant measure/show the uncertainty.\n\nYou want to know the probability of the coin being biased to 0.75 heads, given the data of 9 heads in 12 trials. Theta represents the model, or (unknown) probability.\n\ni.e. $p(Heads\\mid N,\\theta)$\n\nAnd the number of heads will pertain to the binomial distribution:\n\n$heads \\sim Binomial(N,\\theta)$\n\n\n::: {.cell output.var='heads'}\n\n```{.stan .cell-code}\n\ndata { //known observation data\n  int<lower=0> h;\n  int<lower=0> N;\n}\n\nparameters {\n  real<lower=0, upper=1> theta;\n}\n\nmodel {\n  // theta ~ uniform(0,1);\n  h ~ binomial(N, theta);\n}\n\ngenerated quantities {\n  real log_lik;\n  int heads;\n  \n  log_lik = binomial_lpmf(h | N, theta);\n  heads = binomial_rng(N, theta);\n  \n}\n```\n:::\n\n\nStan uses Markov Chain Monte Carlo sampling, to estimate the parameter. One chain is run per CPU core. Each chain samples a point on the distribution. It is rewarded for samples near the peak/mean of the distribution. It then moves to another area of the distribution. In this instance, it does this 1000 times. Given the reward, it thus converges around this mean, which you can see in the trace plot.\n\nThis probability density function (PDF), graphically shows the uncertainty contained within the parameter estimate. The red area denotes the highest density interval (HDI), set here to its default of 0.8/80%. If more data were introduced, the uncertainty would reduce.\n\nStan can also generate data based upon the parameter estimate, from the 4\\*1000 samples. The histogram shows 9 heads being the most likely based on the 4\\*1000 samples.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'a57dca310d1928b2ee4bc7cb5276c4f4' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.006199 seconds (Warm-up)\nChain 1:                0.006124 seconds (Sampling)\nChain 1:                0.012323 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'a57dca310d1928b2ee4bc7cb5276c4f4' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 3e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.006536 seconds (Warm-up)\nChain 2:                0.006617 seconds (Sampling)\nChain 2:                0.013153 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'a57dca310d1928b2ee4bc7cb5276c4f4' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 3e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.006694 seconds (Warm-up)\nChain 3:                0.006323 seconds (Sampling)\nChain 3:                0.013017 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'a57dca310d1928b2ee4bc7cb5276c4f4' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.006556 seconds (Warm-up)\nChain 4:                0.006083 seconds (Sampling)\nChain 4:                0.012639 seconds (Total)\nChain 4: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n$summary\n              mean     se_mean        sd        2.5%        25%        50%\ntheta    0.7132458 0.002977411 0.1173131   0.4622676  0.6346924  0.7232075\nlog_lik -1.8128515 0.015684167 0.6319924  -3.5720612 -1.9772618 -1.5695601\nheads    8.5695000 0.044862658 2.0776787   4.0000000  7.0000000  9.0000000\nlp__    -8.9031007 0.018172299 0.7260527 -11.0087807 -9.0722529 -8.6189080\n               75%      97.5%    n_eff     Rhat\ntheta    0.7994954  0.9087166 1552.443 1.004564\nlog_lik -1.4005660 -1.3549262 1623.681 1.002898\nheads   10.0000000 12.0000000 2144.800 1.002835\nlp__    -8.4350018 -8.3762300 1596.307 1.003694\n\n$c_summary\n, , chains = chain:1\n\n         stats\nparameter       mean        sd        2.5%        25%        50%        75%\n  theta    0.7034403 0.1220785   0.4499461  0.6207978  0.7150049  0.7961614\n  log_lik -1.8495497 0.6713904  -3.6680190 -2.0153223 -1.6051594 -1.4084611\n  heads    8.4690000 2.1108042   4.0000000  7.0000000  9.0000000 10.0000000\n  lp__    -8.9207156 0.7102808 -10.9576006 -9.1610119 -8.6449259 -8.4421599\n         stats\nparameter      97.5%\n  theta    0.8968506\n  log_lik -1.3548804\n  heads   12.0000000\n  lp__    -8.3762945\n\n, , chains = chain:2\n\n         stats\nparameter       mean        sd        2.5%        25%        50%        75%\n  theta    0.7080988 0.1166642   0.4570998  0.6335021  0.7177761  0.7927339\n  log_lik -1.8120667 0.6505674  -3.6454792 -1.9772742 -1.5570039 -1.3985253\n  heads    8.4890000 2.0890771   4.0000000  7.0000000  9.0000000 10.0000000\n  lp__    -8.8874360 0.7311442 -10.7931361 -9.0483744 -8.5967650 -8.4265006\n         stats\nparameter      97.5%\n  theta    0.9017759\n  log_lik -1.3548051\n  heads   12.0000000\n  lp__    -8.3761784\n\n, , chains = chain:3\n\n         stats\nparameter       mean        sd        2.5%       25%        50%       75%\n  theta    0.7130037 0.1120249   0.4731105  0.636666  0.7214262  0.791345\n  log_lik -1.7785873 0.5933537  -3.4430740 -1.932749 -1.5431655 -1.395346\n  heads    8.5680000 1.9766849   4.0000000  7.000000  9.0000000 10.000000\n  lp__    -8.8597447 0.7001664 -10.8360958 -9.012821 -8.5883750 -8.427869\n         stats\nparameter      97.5%\n  theta    0.9062153\n  log_lik -1.3548590\n  heads   12.0000000\n  lp__    -8.3762556\n\n, , chains = chain:4\n\n         stats\nparameter       mean        sd        2.5%        25%        50%        75%\n  theta    0.7284403 0.1169341   0.4772971  0.6476205  0.7393609  0.8178019\n  log_lik -1.8112024 0.6084888  -3.4942731 -1.9486326 -1.5777260 -1.4043339\n  heads    8.7520000 2.1220271   4.0000000  7.0000000  9.0000000 10.0000000\n  lp__    -8.9445064 0.7594309 -11.2375994 -9.1174701 -8.6363056 -8.4409566\n         stats\nparameter      97.5%\n  theta    0.9191709\n  log_lik -1.3552179\n  heads   12.0000000\n  lp__    -8.3761907\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nci_level: 0.8 (80% intervals)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nouter_level: 0.95 (95% intervals)\n```\n:::\n\n::: {.cell-output-display}\n![](RL_23.01.23_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](RL_23.01.23_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](RL_23.01.23_files/figure-html/unnamed-chunk-4-3.png){width=672}\n:::\n:::\n\n\nNow to look at FLARe data, which, while more complex than a coin flip, shares some similarities. There is an unknown parameter to the participants, the reinforcement rate of the US (75%). They experience the US as happening or not happening, so could be said to be binomial in distribution. This happens over 12 trials.\n\nThe first plot shows 10 participants individual ratings over 12 trials, and the group mean in black. This individual variation is the thing I hope models can explain.\n\nThe second plot shows a single participant. This participant appears to do the experiment correctly, and reacts as expected to US. It also appears similar to the mean.\n\nFor now, for simplicity, I am just going to model the CS+ acquisition.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](RL_23.01.23_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 3 rows containing missing values (geom_point).\n```\n:::\n\n::: {.cell-output-display}\n![](RL_23.01.23_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n# Simulation of RW and learning rates in R\n\nSimple graphs of RW updating.\n\nTop left graph is the actual data, plotted.\n\nThe three other graphs show the value updating, given low, medium, and high learning rates.\n\n# The RW model\n\n$\\alpha$ - Learning rate (free parameter)\n\n$PE$ - reward prediction error (reward - current expectation)\n\n$V$ - value (subjective)\n\n$R$ - reward (US = 1, no US = 0)\n\n$t$ - trial (1,2,...,12)\n\n$\\text{Value Update: } V_t = V_{t-1}+\\alpha*PE_{t-1}$\n\n$\\text{Prediction Error: } PE_{t-1} = R_{t-1} - V_{t-1}$\n\n$V_t = V_{t-1} + \\alpha(R_{t-1} - V_{t-1})$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](RL_23.01.23_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n# Data processing steps\n\nExpectancy rating of 1, 2, ..., 9 is categorical. I have scaled this to a continuous distribution between 0 and 1, to represent (participant's subjective) probability.\n\nScale is 9 steps between $\\frac{0.5}{9}$ and $1$ in $\\frac{1}{9}$ increments.\n\nUS / Scream is considered a 'reward', and has value of $1$. No US has a value of $0$.\n\n------------------------------------------------------------------------\n\n# Model 1 - Rescorla Wagner, Choice \\~ Normal(value,sigma)\n\n### Model and Parameters\n\nRW value updating, via learning rate parameter $lr$.\n\nValue mapped to participant choice / expectancy rating via a normal distribution. Mean $\\mu$ is value for that trial. Sigma $\\sigma$ is estimated by the model.\n\n### Outputs\n\nLog Likelihood extracted per participant and trial. Leave one out cross validation (LOOCV) and WAIC calculated from this. (For model comparison)\n\nHistogram of generated data for trial 9, showing normal distribution.\n\nFinal plot is mean of generated choices from model in blue, and actual choice data, all from participant one. Not sure how to use this data yet, possibly some sort of divergance score.\n\n\n::: {.cell output.var='normal'}\n\n```{.stan .cell-code}\n\ndata {\n  int<lower=1> ntrials;          \n  int<lower=1> nsub;\n  real<lower=0,upper=1> choice[nsub,ntrials]; //array of size nTrials (12 intigers)\n  real<lower=-1, upper=1> reward[nsub,ntrials]; \n}\n\nparameters {\n  real<lower=0,upper=1> lr[nsub];\n  real<lower=0> sigma[nsub];  \n}\n\nmodel {\n\n  real v[nsub,ntrials];\n  real pe[nsub,ntrials];       // prediction error\n  \n  for (s in 1:nsub) {\n  v[s,1] = 0.5;\n\n  for (t in 1:(ntrials-1)) { \n    \n    pe[s,t] = reward[s,t] - v[s,t];\n\n    v[s,t+1] = v[s,t] + lr[s] * pe[s,t]; \n  }  \n  \n  for (t in 1:ntrials) {\n\n    choice[s,t] ~ normal (v[s,t],sigma[s]);\n  }\n}\n}\n\ngenerated quantities {\n  real log_lik[nsub,ntrials];\n  real choice_pred[nsub,ntrials];\n  \n  choice_pred = rep_array(-999,nsub,ntrials);\n  \n  {\n  real v[nsub,ntrials];\n  real pe[nsub,ntrials];      \n  \n  for (s in 1:nsub) {\n  v[s,1] = 0.5;\n\n  for (t in 1:(ntrials-1)) { \n    \n    pe[s,t] = reward[s,t] - v[s,t];\n\n    v[s,t+1] = v[s,t] + lr[s] * pe[s,t]; \n  }  \n  \n  for (t in 1:ntrials) {\n    log_lik[s,t] = normal_lpdf(choice[s,t] | v[s,t],sigma[s]);\n    choice_pred[s,t] = normal_rng(v[s,t],sigma[s]);\n  }\n}\n  }\n}\n\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL '5875f43ba02dc332fb6935055510e2bc' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.5 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.234605 seconds (Warm-up)\nChain 1:                0.183151 seconds (Sampling)\nChain 1:                0.417756 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL '5875f43ba02dc332fb6935055510e2bc' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2.2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.23803 seconds (Warm-up)\nChain 2:                0.18511 seconds (Sampling)\nChain 2:                0.42314 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL '5875f43ba02dc332fb6935055510e2bc' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.238742 seconds (Warm-up)\nChain 3:                0.180912 seconds (Sampling)\nChain 3:                0.419654 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL '5875f43ba02dc332fb6935055510e2bc' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 2.2e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.247028 seconds (Warm-up)\nChain 4:                0.185413 seconds (Sampling)\nChain 4:                0.432441 seconds (Total)\nChain 4: \n```\n:::\n\n::: {.cell-output-display}\n![](RL_23.01.23_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nci_level: 0.8 (80% intervals)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nouter_level: 0.95 (95% intervals)\n```\n:::\n\n::: {.cell-output-display}\n![](RL_23.01.23_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nci_level: 0.8 (80% intervals)\nouter_level: 0.95 (95% intervals)\n```\n:::\n\n::: {.cell-output-display}\n![](RL_23.01.23_files/figure-html/unnamed-chunk-8-3.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Relative effective sample sizes ('r_eff' argument) not specified.\nFor models fit with MCMC, the reported PSIS effective sample sizes and \nMCSE estimates will be over-optimistic.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: \n9 (7.5%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n```\n:::\n\n::: {.cell-output-display}\n![](RL_23.01.23_files/figure-html/unnamed-chunk-8-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](RL_23.01.23_files/figure-html/unnamed-chunk-8-5.png){width=672}\n:::\n:::\n\n\n# Model 2 - Rescorla Wagner, Choice \\~ Beta(alpha,beta)\n\n### Model and Parameters\n\nRW value updating, via learning rate parameter $lr$.\n\nValue mapped to participant choice / expectancy rating via a beta distribution. Mean $\\mu$ is value for that trial. Sigma $\\sigma$ is estimated by the model. (and called beta)\n\nBeta distribution is more flexible than normal. It is governed by two shaping parameters, alpha and beta. These can be derived from mean and variance:\n\n$\\alpha = \\mu(\\frac{\\mu(1 - \\mu)}{\\sigma - 1})$\n\n$\\beta = (1 - \\mu)(\\frac{\\mu(1 - \\mu)}{\\sigma - 1})$\n\n### Outputs\n\nAs model 1. Log Likelihood extracted per participant and trial. Leave one out cross validation (LOOCV) and WAIC calculated from this. (For model comparison)\n\nHistogram showing generated data of trial 9, showing beta distribution, skewed towards high probability. \n\nFinal plot is mean of generated choices from model in blue, and actual choice data, all from participant one.\n\n\n::: {.cell output.var='beta'}\n\n```{.stan .cell-code}\n\ndata {\n  int<lower=1> ntrials;          \n  int<lower=1> nsub;\n  real<lower=0,upper=1> choice[nsub,ntrials]; //array of size nTrials (12 intigers)\n  real<lower=-1, upper=1> reward[nsub,ntrials]; \n}\n\nparameters {\n  real<lower=0,upper=1> lr[nsub];\n  real<lower=0> beta[nsub];  \n}\n\nmodel {\n\n  real v[nsub,ntrials];\n  real pe[nsub,ntrials];       // prediction error\n  \n  real shape_alpha[nsub,ntrials];\n  real shape_beta[nsub,ntrials];\n  \n  //beta ~ normal(0,1);\n  \n  for (s in 1:nsub) {\n  v[s,1] = 0.5;\n\n  for (t in 1:(ntrials-1)) { \n    \n    pe[s,t] = reward[s,t] - v[s,t];\n\n    v[s,t+1] = v[s,t] + lr[s] * pe[s,t]; \n  }  \n  \n  for (t in 1:ntrials) {\n    \n    shape_alpha[s,t] = v[s,t] * ((v[s,t] * (1-v[s,t]) / beta[s]));\n    shape_beta[s,t] = (1-v[s,t]) * ((v[s,t] * (1-v[s,t]) / beta[s]));\n    \n    choice[s,t] ~ beta(shape_alpha[s,t],shape_beta[s,t]);\n  }\n}\n}\n\ngenerated quantities {\n  real log_lik[nsub,ntrials];\n  real choice_pred[nsub,ntrials];\n  \n  choice_pred = rep_array(-999,nsub,ntrials);\n  \n  {\n  real v[nsub,ntrials];\n  real pe[nsub,ntrials];      \n  \n  real shape_alpha[nsub,ntrials];\n  real shape_beta[nsub,ntrials];\n  \n  for (s in 1:nsub) {\n  v[s,1] = 0.5;\n\n  for (t in 1:(ntrials-1)) { \n    \n    pe[s,t] = reward[s,t] - v[s,t];\n\n    v[s,t+1] = v[s,t] + lr[s] * pe[s,t]; \n  }  \n  \n  for (t in 1:ntrials) {\n    shape_alpha[s,t] = v[s,t] * ((v[s,t] * (1-v[s,t]) / beta[s]));\n    shape_beta[s,t] = (1-v[s,t]) * ((v[s,t] * (1-v[s,t]) / beta[s]));\n    \n    log_lik[s,t] = beta_lpdf(choice[s,t] | shape_alpha[s,t], shape_beta[s,t]);\n    choice_pred[s,t] = beta_rng(shape_alpha[s,t], shape_beta[s,t]);\n  }\n}\n  }\n}\n\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'f2f34ad5742d062c2187462b6d9968c2' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000107 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.07 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.688469 seconds (Warm-up)\nChain 1:                0.679535 seconds (Sampling)\nChain 1:                1.368 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'f2f34ad5742d062c2187462b6d9968c2' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5.9e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.59 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.650694 seconds (Warm-up)\nChain 2:                0.635476 seconds (Sampling)\nChain 2:                1.28617 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'f2f34ad5742d062c2187462b6d9968c2' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 5e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.5 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.65126 seconds (Warm-up)\nChain 3:                0.681068 seconds (Sampling)\nChain 3:                1.33233 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'f2f34ad5742d062c2187462b6d9968c2' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 5.3e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.53 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.654405 seconds (Warm-up)\nChain 4:                0.621782 seconds (Sampling)\nChain 4:                1.27619 seconds (Total)\nChain 4: \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n'pars' not specified. Showing first 10 parameters by default.\n```\n:::\n\n::: {.cell-output-display}\n![](RL_23.01.23_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nci_level: 0.8 (80% intervals)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nouter_level: 0.95 (95% intervals)\n```\n:::\n\n::: {.cell-output-display}\n![](RL_23.01.23_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nci_level: 0.8 (80% intervals)\nouter_level: 0.95 (95% intervals)\n```\n:::\n\n::: {.cell-output-display}\n![](RL_23.01.23_files/figure-html/unnamed-chunk-10-3.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Relative effective sample sizes ('r_eff' argument) not specified.\nFor models fit with MCMC, the reported PSIS effective sample sizes and \nMCSE estimates will be over-optimistic.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Some Pareto k diagnostic values are slightly high. See help('pareto-k-diagnostic') for details.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: \n4 (3.3%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n```\n:::\n\n::: {.cell-output-display}\n![](RL_23.01.23_files/figure-html/unnamed-chunk-10-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](RL_23.01.23_files/figure-html/unnamed-chunk-10-5.png){width=672}\n:::\n:::\n\n\nCompare the two models. Beta model has lower WAIC, so is the superior model.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n       elpd_diff se_diff\nmodel2   0.0       0.0  \nmodel1 -34.8       5.7  \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Accessing waic using '$' is deprecated and will be removed in a\nfuture release. Please extract the waic estimate from the 'estimates' component\ninstead.\n\nWarning: Accessing waic using '$' is deprecated and will be removed in a\nfuture release. Please extract the waic estimate from the 'estimates' component\ninstead.\n```\n:::\n\n::: {.cell-output-display}\n![](RL_23.01.23_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Accessing waic using '$' is deprecated and will be removed in a\nfuture release. Please extract the waic estimate from the 'estimates' component\ninstead.\n\nWarning: Accessing waic using '$' is deprecated and will be removed in a\nfuture release. Please extract the waic estimate from the 'estimates' component\ninstead.\n```\n:::\n\n::: {.cell-output-display}\n![](RL_23.01.23_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n:::\n\n\n<!-- Ridge plots to show estimated distribution density. Shows the advantage of beta distribution over normal, which is inflexible and inappropriate given upper bounds of probability at 1. -->\n\n<!-- ```{r} -->\n\n<!-- gen_data <- extract(fit_normal) -->\n<!-- gen_data <- gen_data$choice_pred -->\n\n<!-- library(ggridges) -->\n<!-- ggplot(gen_data_tibble, aes(x = gen_data_tibble[,1,], y = gen_data_tibble[,1,1:12])) + geom_density_ridges() -->\n\n<!-- gen_data_tib <- as.tibble(gen_data[,1,]) -->\n<!-- tib2 <- gen_data_tib %>% pivot_longer(cols = starts_with('V'), names_to = 'Trial', values_to = 'Rating') -->\n<!-- tib2$Trial <- gsub('V10','W10',as.character(tib2$Trial)) -->\n<!-- tib2$Trial <- gsub('V11','W11',as.character(tib2$Trial)) -->\n<!-- tib2$Trial <- gsub('V12','W12',as.character(tib2$Trial)) -->\n<!-- # tib2$Trial <- as.integer(tib2$Trial) -->\n\n<!-- single_subject_expectancy_tib <- single_subject_expectancy %>% as.tibble() %>% pivot_longer(cols = starts_with('X'), names_to = 'Trial', values_to = 'Rating') -->\n<!-- single_subject_expectancy_tib$Trial <- gsub('X10','Y10',as.character(single_subject_expectancy_tib$Trial)) -->\n<!-- single_subject_expectancy_tib$Trial <- gsub('X11','Y11',as.character(single_subject_expectancy_tib$Trial)) -->\n<!-- single_subject_expectancy_tib$Trial <- gsub('X12','Y12',as.character(single_subject_expectancy_tib$Trial)) -->\n\n<!-- ggplot(tib2, aes(x = Trial, y = Rating, group = Trial)) + geom_density_ridges(aes(fill = Trial)) -->\n\n<!-- choice_1 <- scale_er(choice) -->\n<!-- choice_1_df <- data.frame(Trial = c('V1','V2','V3','V4','V5','V6','V7','V8','V9','W10','W11','W12'), Rating = choice_1 )  -->\n\n<!-- ggplot(tib2, aes(y = Trial, x = Rating)) + geom_density_ridges(scale = 1) + geom_point(data = choice_1_df, col = 'blue') + xlim(0,1.2) -->\n\n<!-- order <- c('V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12') -->\n\n<!-- mean(gen_data_tib$V2) -->\n\n<!-- ``` -->\n\n<!-- ```{r} -->\n\n<!-- gen_data <- extract(fit_beta) -->\n<!-- gen_data <- gen_data$choice_pred -->\n\n<!-- library(ggridges) -->\n<!-- ggplot(gen_data_tibble, aes(x = gen_data_tibble[,1,], y = gen_data_tibble[,1,1:12])) + geom_density_ridges() -->\n\n<!-- gen_data_tib <- as.tibble(gen_data[,1,]) -->\n<!-- tib2 <- gen_data_tib %>% pivot_longer(cols = starts_with('V'), names_to = 'Trial', values_to = 'Rating') -->\n<!-- tib2$Trial <- gsub('V10','W10',as.character(tib2$Trial)) -->\n<!-- tib2$Trial <- gsub('V11','W11',as.character(tib2$Trial)) -->\n<!-- tib2$Trial <- gsub('V12','W12',as.character(tib2$Trial)) -->\n<!-- # tib2$Trial <- as.integer(tib2$Trial) -->\n\n<!-- single_subject_expectancy_tib <- single_subject_expectancy %>% as.tibble() %>% pivot_longer(cols = starts_with('X'), names_to = 'Trial', values_to = 'Rating') -->\n<!-- single_subject_expectancy_tib$Trial <- gsub('X10','Y10',as.character(single_subject_expectancy_tib$Trial)) -->\n<!-- single_subject_expectancy_tib$Trial <- gsub('X11','Y11',as.character(single_subject_expectancy_tib$Trial)) -->\n<!-- single_subject_expectancy_tib$Trial <- gsub('X12','Y12',as.character(single_subject_expectancy_tib$Trial)) -->\n\n<!-- ggplot(tib2, aes(x = Trial, y = Rating, group = Trial)) + geom_density_ridges(aes(fill = Trial)) -->\n\n<!-- choice_1 <- scale_er(choice) -->\n<!-- choice_1_df <- data.frame(Trial = c('V1','V2','V3','V4','V5','V6','V7','V8','V9','W10','W11','W12'), Rating = choice_1 )  -->\n\n<!-- ggplot(tib2, aes(y = Trial, x = Rating)) + geom_density_ridges(scale = 1) + geom_point(data = choice_1_df, col = 'blue') + xlim(0,1.2) -->\n\n<!-- order <- c('V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12') -->\n\n<!-- mean(gen_data_tib$V2) -->\n<!-- ``` -->\n",
    "supporting": [
      "RL_23.01.23_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}