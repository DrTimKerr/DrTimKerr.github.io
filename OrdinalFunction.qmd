---
title: "My Ordinal Function"
---
```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(LaplacesDemon)
library(ggplot2)
library(patchwork)

softmax <- function(par){
  n.par <- length(par)
  par1 <- sort(par, decreasing = TRUE)
  Lk <- par1[1]
  for (k in 1:(n.par-1)) {
    Lk <- max(par1[k+1], Lk) + log1p(exp(-abs(par1[k+1] - Lk))) 
  }
  val <- exp(par - Lk)
  return(val)
}

```

This is a function that links associative value to a probability distribution of choices. Given the choices are ordinal, a likert scale of 1 to 9, where 8 is larger than 7, but is not necessarily 8 times larger than 1, or twice as large as 4. 

In neural networks, they sometimes use regularisation, to make the one-hot encoding less hot.

$$\begin{align} \alpha & = \text{A parameter which sets the hotness} \\
K & = \text{The number of choices}
\end{align}$$

$$ \huge p = (1-\alpha) ~\cdot~[0,0,1] ~ +~ \frac{\alpha}{K}  $$
```{r,echo = FALSE, message = FALSE, warning = FALSE}
alpha <- 0.2
K <- 9
y_hot <- c(0,0,0,0,1,0,0,0,0) # I guess port 0.1 as c(1,0...)

p <- (1-alpha) * y_hot + alpha / K

alpha <- 0.8
p2 <- (1-alpha) * y_hot + alpha / K

data1=data.frame(value=rcat(1000,c(softmax(p))))
data2=data.frame(value=rcat(1000,c(softmax(p2))))


p1 <- ggplot(data1, aes(x = value)) + 
  geom_histogram(aes(y = ..density..), binwidth = 1 ,colour="black", fill="white") + 
  geom_density(alpha=0.2, fill = 'red') +
  ggtitle("Alpha = 0.2, V = 0.5")


p2 <- ggplot(data2, aes(x = value)) + geom_histogram(aes(y = ..density..),binwidth = 1, colour="black", fill="white") + geom_density(alpha=0.2, fill = 'red') +
  ggtitle("Alpha = 0.8, V = 0.5")

p1 + p2

# # library(LaplacesDemon)
# par(mfrow=c(1,2))
# hist(rcat(100,c(p)), breaks = 10, main = "Low Alpha", xlab = "Choice")
# hist(rcat(100,c(p2)), breaks = 10, main = "High Alpha",xlab = "Choice")
```

Its still quite binary, and doesn't capture any ordinal features.

Really we are looking for a unimodal distribution, centred around the associative value.

There are a few suggested ways of doing this. Poisson distribtions are not flexible enough. Binomial distributions dont work at extreme values. The beta distribution could work, but is hard to parameterise with only a mean or mode.

But from this paper, I found an alternative solution.

https://www.sciencedirect.com/science/article/pii/S0925231220300618?via%3Dihub

$$ \tau = \text{Our inverse temperature parameter, essentially choice precision}$$ 

$$\begin{align} \huge p_i~ &\huge= exp\left( - \left| \frac{ \frac{i}{K} - V_t}{\tau}\right|\right) \left\{i \in \mathbb{Z}~ | ~1 \leqslant i \leqslant K \right\} \\ \huge p~ &\huge =  \text{Softmax}\left(p_1,p_2,...,p_K\right)
\end{align}
$$


By subtracting a label from ground truth, you see a measure of distance from ground truth. In this instance, associative value is ground truth, and the labels are K equal points along the unit interval. In FLARe this will be 9, to represent the nine possilbe choices on the scale.

```{r,echo = FALSE, message = FALSE, warning = FALSE}

p <- rep(0,9)
T <- 0.1
l <- 0.7
for (i in 1:9){
p[i] <- exp(-abs(i/9-l)/T)
}

p2 <- rep(0,9)
T <- 0.8

for (i in 1:9){
p2[i] <- exp(-abs(i/9-l)/T)
}

p3 <- rep(0,9)
T <- 5

for (i in 1:9){
p3[i] <- exp(-abs(i/9-l)/T)
}

data1=data.frame(value=rcat(1000,c(softmax(p))))
data2=data.frame(value=rcat(1000,c(softmax(p2))))
data3=data.frame(value=rcat(1000,c(softmax(p3))))


p1 <- ggplot(data1, aes(x = value)) + 
  geom_histogram(aes(y = ..density..), binwidth = 1 ,colour="black", fill="white") + 
  geom_density(alpha=0.2, fill = 'red') +
  ggtitle("Tau = 0.1, V = 0.7")


p2 <- ggplot(data2, aes(x = value)) + geom_histogram(aes(y = ..density..),binwidth = 1, colour="black", fill="white") + geom_density(alpha=0.2, fill = 'red') +
  ggtitle("Tau = 0.8, V = 0.7")


p3 <- ggplot(data3, aes(x = value)) + geom_histogram(aes(y = ..density..),binwidth = 1, colour="black", fill="white") + geom_density(alpha=0.2, fill = 'red') +
  ggtitle("Tau = 5, V = 0.7")


p1 + p2 + p3


```
A low $\tau$ means a more precise choice, with further away choices much less likely than near choices (left). A higher $\tau$ means nearer choices are almost as likely, with further away choices still unlikely (middle). Finally a very high $\tau$ makes the distribution almost uniform, per the normal regularisation (right).
